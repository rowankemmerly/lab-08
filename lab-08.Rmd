---
title: "Lab 08 - University of Edinburgh Art Collection"
author: "Rowan Kemmerly"
date: "5/3/2023"
output: github_document
---

### Load packages and data

```{r load-packages, message = FALSE}
library(tidyverse) 
library(skimr)
```

```{r load-data, message = FALSE, eval = FALSE}
# Remove eval = FALSE or set it to TRUE once data is ready to be loaded
uoe_art <- read_csv("data/uoe-art.csv")
```

### Exercise 1

The actual link has "https://collections.ed.ac.uk/art" before the"/record" part, so I've fixed the URLs via str_replace to reflect that. 

```{r scrape-links}
links <- page %>%
  html_nodes(".iteminfo") %>%
  html_node("h3 a") %>%
  html_attr("href") %>%
  str_replace("\\.", "https://collections.ed.ac.uk/art")
```

### Exercise 2

```{r scrape-artist-names}
artists <- page %>%
  html_nodes(".iteminfo") %>%
  html_node(".artist") %>%
  html_text() 
```

### Exercise 3

```{r create-tibble}
first_ten <- tibble(
  title = titles,
  artist = artists,
  link = links
)
```

### Exercise 4

```{r second-url}
# set url ----------------------------------------------------------------------

second_url <- "https://collections.ed.ac.uk/art/search/*:*/Collection:%22edinburgh+college+of+art%7C%7C%7CEdinburgh+College+of+Art%22?offset=10"

# read second page --------------------------------------------------------------

page <- read_html(second_url)

# scrape titles ----------------------------------------------------------------

titles <- page %>%
  html_nodes(".iteminfo") %>%
  html_node("h3 a") %>%
  html_text() %>%
  str_squish()

# scrape links -----------------------------------------------------------------

links <- page %>%
  html_nodes(".iteminfo") %>%
  html_node("h3 a") %>%
  html_attr("href") %>%
  str_replace("\\.", "https://collections.ed.ac.uk/art")

# scrape artists ---------------------------------------------------------------

artists <- page %>%
  html_nodes(".iteminfo") %>%
  html_node(".artist") %>%
  html_text()

# put together in a data frame -------------------------------------------------

second_ten <- tibble(
  title = titles,
  artist = artists,
  link = links
)
```


### Exercise 5

```{r scrape-page-function}
scrape_page <- function(url){
  
  # read page
  page <- read_html(url)
  
  # scrape titles
  titles <- page %>%
    html_nodes(".iteminfo") %>%
    html_node("h3 a") %>%
    html_text() %>%
    str_squish()
  
  # scrape links
  links <- page %>%
    html_nodes(".iteminfo") %>%
    html_node("h3 a") %>%
    html_attr("href") %>%
    str_replace("\\.", "https://collections.ed.ac.uk/art")
  
  # scrape artists 
  artists <- page %>%
    html_nodes(".iteminfo") %>%
    html_node(".artist") %>%
    html_text()
  
  # create and return tibble
  tibble(
    title = titles,
    artist = artists,
    link = links
  )
  
}
```


### Exercise 6

```{r trying-out-function}

scrape_page(first_url)
scrape_page(second_url)

```

Yes, the output looks good to me!


### Exercise 7

```{r iteration}
root <- "https://collections.ed.ac.uk/art/search/*:*/Collection:%22edinburgh+college+of+art%7C%7C%7CEdinburgh+College+of+Art%22?offset="
numbers <- seq(from = 0, to = 2900, by = 10)
urls <- glue("{root}{numbers}")
```


### Exercise 8

```{r creating-new-df}
uoe_art <- map_dfr(urls, scrape_page)
```


### Exercise 9

```{r new-df}
write_csv(uoe_art, file = "data/uoe-art.csv")

uoe_art <- read_csv("data/uoe-art.csv")

uoe_art
```

For whatever reason there are only 982 pieces of art showing up and not 3017! (I took a screenshot of this on the UOE website that's in this repo.)


### Exercise 10

```{r separate-title-date, error = TRUE}
uoe_art <- uoe_art %>%
  separate(title, into = c("title", "date"), sep = "\\(") %>%
  mutate(year = str_remove(date, "\\)") %>% as.numeric()) %>%
  select(title, artist, year, link)
```

So it looks like it's throwing out information beyond title, artist, year, and link, but I guess that's okay since we just want the titles and years. And it's talking about how for some pieces there is no date so date is "NA"â€”we knew this already. 


### Exercise 11

...

Add exercise headings as needed.
